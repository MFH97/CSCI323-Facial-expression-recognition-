{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Group Assignment**\n",
        "\n",
        "**T04 Group 45**\n",
        "\n",
        "**Facial Expression Recognition**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QJY5gmmA39EE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Facial Expression Recognition (FER) is a crucial task in computer vision, with applications ranging from emotion analysis to human-computer interaction. This project explores the effectiveness of different neural network architectures (CNN, VGG16, ResNet) and baseline models (KNN, SVM, Random Forest) for recognizing facial expressions. The dataset used for this project is taken from FER-2013, it consists of grayscale images of faces, each labeled with one of several emotions."
      ],
      "metadata": {
        "id": "o1LIeuc_kb4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5UGBk6z0HFa"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Preprocessing the data"
      ],
      "metadata": {
        "id": "tYc2MMDY0jf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is stored in a google drive directory where each subdirectory represents a class label (e.g., \"happy,\" \"sad\"). The images are loaded using OpenCV in grayscale mode, resized to 48x48 pixels, and normalized to have pixel values between 0 and 1. This preprocessing step ensures that the images are uniform in size and the model training process is stable and efficient.\n",
        "\n",
        "The labels, originally in string format (e.g., \"happy,\" \"sad\"), are converted to numerical values using label encoding, followed by one-hot encoding to make them suitable for multi-class classification. The dataset is then split into training, validation, and test sets, ensuring that the models can be evaluated on unseen data."
      ],
      "metadata": {
        "id": "zyXsHJhalKZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Define the path to the dataset\n",
        "data_dir = '/content/drive/MyDrive/archive/Fer2013_train'\n",
        "\n",
        "# Initialize lists to store images and their corresponding labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Load and preprocess images by\n",
        "# Looping through each label directory in the dataset\n",
        "for label in os.listdir(data_dir):\n",
        "    class_dir = os.path.join(data_dir, label)\n",
        "    if os.path.isdir(class_dir): # Ensure it's a directory\n",
        "        for img_file in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_file)\n",
        "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # Load the image in grayscale mode\n",
        "            img = cv2.resize(img, (48, 48))  # Resize to the desired size (48x48)\n",
        "            img = img / 255.0  # Normalize the image to [0, 1]\n",
        "            images.append(img) # Append the processed image to the images list\n",
        "            labels.append(label) # Append the label to the labels list\n",
        "\n",
        "# Convert the lists of images and labels into NumPy arrays\n",
        "X = np.array(images).reshape(-1, 48, 48, 1) # Reshape images to include channel dimension\n",
        "y = np.array(labels)\n",
        "\n",
        "# Encode labels to integers\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y) # Transform labels to integer encoding\n",
        "y = to_categorical(y, num_classes=len(np.unique(y)))# One-hot encode the labels\n",
        "\n",
        "# Splitting into train, validation, and test sets\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "EHwtW0bd0mJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advance Models"
      ],
      "metadata": {
        "id": "Tdfm8XEt0rVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "dOOdiPB60wos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A CNN architecture is implemented with the following layers:\n",
        "\n",
        "\n",
        "\n",
        "*   **Convolutional Layers**: Two convolutional layers with ReLU activation functions extract features from the images.\n",
        "*   **Pooling Layers**: MaxPooling layers reduce the spatial dimensions of the feature maps, helping in down-sampling.\n",
        "*   **Fully Connected Layer**: A Dense layer with 128 neurons, followed by a Dropout layer to prevent overfitting.\n",
        "*   **Output Layer**: A softmax activation function is used to predict the probability distribution across the different classes.\n",
        "\n",
        "The model is compiled using the Adam optimizer and categorical cross-entropy loss, and it is trained for 25 epochs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KF7uN4KGluKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Function to create a CNN model\n",
        "def create_cnn_model():\n",
        "    model = Sequential()\n",
        "     # First convolutional layer with 32 filters, 3x3 kernel, and ReLU activation\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))  # Max pooling with 2x2 pool size\n",
        "    # Second convolutional layer with 64 filters\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2))) # Max pooling with 2x2 pool size\n",
        "    model.add(Flatten()) # Flatten the output from the convolutional layers\n",
        "    # Dense layer with 128 neurons and ReLU activation\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5)) # Dropout to prevent overfitting\n",
        "    # Output layer with softmax activation for multi-class classification\n",
        "    model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Create and compile the CNN model\n",
        "cnn_model = create_cnn_model()\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the CNN model\n",
        "cnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=25, batch_size=64)\n",
        "\n"
      ],
      "metadata": {
        "id": "9VdSW7Iv0wDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16"
      ],
      "metadata": {
        "id": "-ssgdvWt0029"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pre-trained deep convolutional network, is fine-tuned for this task. The model is loaded with pre-trained ImageNet weights, and the top classification layers are removed. The following layers are added:\n",
        "\n",
        "\n",
        "\n",
        "*   **Flatten Layer**: Converts the 3D output of VGG16 to 1D.\n",
        "*   **Dense Layer**: A Dense layer with 256 neurons and ReLU activation.\n",
        "*   Output Layer: A softmax activation function for multi-class classification.\n",
        "\n",
        "\n",
        "To match VGG16's input requirements (3-channel images), the grayscale images are replicated across three channels. The model is then compiled and trained similarly to the CNN model."
      ],
      "metadata": {
        "id": "jFMel2Xoyv9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input,Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "# Function to create a VGG16-based model\n",
        "def create_vgg16_model():\n",
        "    # Load the VGG16 model pretrained on ImageNet, excluding the top classification layers\n",
        "    vgg16 = VGG16(weights='imagenet', include_top=False, input_tensor=Input(shape=(48, 48, 3)))\n",
        "    model = Sequential()\n",
        "    model.add(vgg16) # Add the VGG16 model as the base\n",
        "    model.add(Flatten()) # Flatten the output of VGG16\n",
        "    # Dense layer with 256 neurons and ReLU activation\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5)) # Dropout to prevent overfitting\n",
        "    # Output layer with softmax activation for multi-class classification\n",
        "    model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Convert grayscale images to 3 channels to match VGG16 input requirements\n",
        "X_train_vgg16 = np.repeat(X_train, 3, axis=-1)\n",
        "X_val_vgg16 = np.repeat(X_val, 3, axis=-1)\n",
        "X_test_vgg16 = np.repeat(X_test, 3, axis=-1)\n",
        "\n",
        "# Create and compile the VGG16 model\n",
        "vgg16_model = create_vgg16_model()\n",
        "vgg16_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the VGG16 model\n",
        "vgg16_model.fit(X_train_vgg16, y_train, validation_data=(X_val_vgg16, y_val), epochs=25, batch_size=64)\n",
        "\n"
      ],
      "metadata": {
        "id": "wnHU8Pj403Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50"
      ],
      "metadata": {
        "id": "e4D9sY-g07BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Another pre-trained deep network, is employed to test the performance of a deeper architecture. Similar to VGG16, ResNet50 is loaded with pre-trained ImageNet weights, and additional layers are added for the classification task. The final architecture is similar to VGG16, with a Flatten layer, a Dense layer, and a softmax output layer."
      ],
      "metadata": {
        "id": "N0cam3vGzV0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input,Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "# Function to create a ResNet-based model\n",
        "def create_resnet_model():\n",
        "    # Load the ResNet50 model pretrained on ImageNet, excluding the top classification layers\n",
        "    resnet = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(48, 48, 3)))\n",
        "    model = Sequential()\n",
        "    model.add(resnet) # Add the ResNet model as the base\n",
        "    model.add(Flatten()) # Flatten the output of ResNet\n",
        "    # Dense layer with 256 neurons and ReLU activation\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5)) # Dropout to prevent overfitting\n",
        "    # Output layer with softmax activation for multi-class classification\n",
        "    model.add(Dense(len(np.unique(labels)), activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Compile and train the ResNet model\n",
        "resnet_model = create_resnet_model()\n",
        "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "resnet_model.fit(X_train_resnet, y_train, validation_data=(X_val_resnet, y_val), epochs=25, batch_size=64)\n"
      ],
      "metadata": {
        "id": "SXS0jKqq09OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Calculation"
      ],
      "metadata": {
        "id": "LPimeDAS1T59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each model is evaluated on the test set using several metrics:\n",
        "\n",
        "\n",
        "*   **Accuracy**: The overall correctness of the model's predictions.\n",
        "*   **Precision, Recall, F1 Score**: These metrics provide insight into the model's performance on each class, particularly in imbalanced datasets.\n",
        "*   **Mean Squared Error (MSE) & Mean Absolute Error (MAE)**: These metrics measure the average errors in the model’s predictions.\n",
        "*   **Cross-Entropy Loss**: This loss measures the difference between the true labels and the predicted probabilities.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WOJ16pzb0LdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, accuracy_score, f1_score, mean_squared_error, mean_absolute_error, log_loss\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate and print various metrics\n",
        "def calculate_metrics(model, X_test, y_test):\n",
        "    # Get predictions\n",
        "    y_pred_prob = model.predict(X_test)  # Predicted probabilities\n",
        "    y_pred = np.argmax(y_pred_prob, axis=1)  # Predicted class labels\n",
        "    y_true = np.argmax(y_test, axis=1)  # True class labels\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    cross_entropy = log_loss(y_true, y_pred_prob)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
        "    print(f'Mean Absolute Error (MAE): {mae:.4f}')\n",
        "    print(f'Cross-Entropy Loss: {cross_entropy:.4f}')\n",
        "\n",
        "# Calculate and print metrics for each model\n",
        "print(\"Metrics for CNN Model:\")\n",
        "calculate_metrics(cnn_model, X_test, y_test)\n",
        "\n",
        "print(\"Metrics for VGG16 Model:\")\n",
        "calculate_metrics(vgg16_model, X_test_vgg16, y_test)\n",
        "\n",
        "print(\"Metrics for ResNet Model:\")\n",
        "calculate_metrics(resnet_model,X_test_resnet, y_test)"
      ],
      "metadata": {
        "id": "Mj3KUyzS1WTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Models"
      ],
      "metadata": {
        "id": "2vUxiMNE0Vmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To provide a comparative analysis, three baseline models are implemented:\n",
        "\n",
        "\n",
        "\n",
        "*  **K-Nearest Neighbors (KNN)**: This model classifies an image based on the majority class among its k-nearest neighbors in the feature space.\n",
        "*   **Support Vector Machine (SVM)**: An SVM model is trained to find the optimal hyperplane that separates the classes in the feature space.\n",
        "*   **Random Forest (RF)**: A Random Forest model, which is an ensemble of decision trees, is trained to classify the images.\n",
        "\n",
        "The images are flattened into 1D vectors, standardized, and then fed into these baseline models."
      ],
      "metadata": {
        "id": "EQNvVRVEzqMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image data generator for dataset loading\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,         # Rescale pixel values to [0, 1]\n",
        "    validation_split=0.2    # Split data into training and validation sets\n",
        ")\n",
        "\n",
        "# Specify the paths to your dataset\n",
        "dataset_path = '/content/drive/MyDrive/Fer2013_train'\n",
        "\n",
        "# Create data generators for training and validation sets\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(48, 48),   # Resize images to 48x48\n",
        "    color_mode='grayscale', # Convert images to grayscale\n",
        "    batch_size=32,          # Number of images to yield per batch\n",
        "    class_mode='categorical', # Use categorical labels (for multi-class classification)\n",
        "    subset='training',      # Set as training data\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(48, 48),   # Resize images to 48x48\n",
        "    color_mode='grayscale', # Convert images to grayscale\n",
        "    batch_size=32,          # Number of images to yield per batch\n",
        "    class_mode='categorical', # Use categorical labels (for multi-class classification)\n",
        "    subset='validation',    # Set as validation data\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Function to extract data from the generator\n",
        "def extract_data(generator):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for i in range(len(generator)):\n",
        "        imgs, lbls = generator[i]\n",
        "        data.extend(imgs)\n",
        "        labels.extend(lbls)\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Extract training and validation data\n",
        "X_train, y_train = extract_data(train_generator)\n",
        "X_val, y_val = extract_data(validation_generator)\n",
        "\n",
        "# Flatten the images for baseline models (KNN, SVM, RF)\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
        "\n",
        "# Convert one-hot labels to categorical labels\n",
        "y_train_cat = np.argmax(y_train, axis=1)\n",
        "y_val_cat = np.argmax(y_val, axis=1)\n",
        "\n",
        "# Standardize the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_flat = scaler.fit_transform(X_train_flat)\n",
        "X_val_flat = scaler.transform(X_val_flat)\n",
        "\n",
        "# Baseline Model: K-Nearest Neighbors (KNN)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_flat, y_train_cat)\n",
        "y_pred_knn = knn.predict(X_val_flat)\n",
        "\n",
        "# Baseline Model: Support Vector Machine (SVM)\n",
        "svm = SVC(probability=True)\n",
        "svm.fit(X_train_flat, y_train_cat)\n",
        "y_pred_svm = svm.predict(X_val_flat)\n",
        "\n",
        "# Baseline Model: Random Forest (RF)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_flat, y_train_cat)\n",
        "y_pred_rf = rf.predict(X_val_flat)\n",
        "\n",
        "# Function to evaluate baseline models\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
        "    # Calculate ROC AUC only for RF and SVM as KNN does not output probabilities\n",
        "    roc_auc = roc_auc_score(y_true, rf.predict_proba(X_val_flat), multi_class='ovr') if model_name == 'RF' else roc_auc_score(y_true, svm.predict_proba(X_val_flat), multi_class='ovr') if model_name == 'SVM' else 'N/A'\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    # Print evaluation metrics\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "    print(f\"{model_name} Precision: {precision}\")\n",
        "    print(f\"{model_name} Recall: {recall}\")\n",
        "    print(f\"{model_name} F1 Score: {f1}\")\n",
        "    print(f\"{model_name} ROC AUC: {roc_auc}\")\n",
        "    print(f\"{model_name} MAE: {mae}\")\n",
        "    print(f\"{model_name} MSE: {mse}\")\n",
        "\n",
        "# Evaluate and print metrics for each baseline model\n",
        "evaluate_model(y_val_cat, y_pred_knn, 'KNN')\n",
        "evaluate_model(y_val_cat, y_pred_svm, 'SVM')\n",
        "evaluate_model(y_val_cat, y_pred_rf, 'RF')"
      ],
      "metadata": {
        "id": "i4NyzGOs0U3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "YZjKt_Aa1EDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A testing module is implemented to allow real-time predictions. Users can upload images, and the trained models will predict the emotion displayed in the image. The function preprocesses the image similarly to the training data and uses the trained models to predict the emotion with an associated confidence score."
      ],
      "metadata": {
        "id": "QRYmpRyF0orX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from google.colab import files\n",
        "\n",
        "# Function to load and preprocess the image for prediction\n",
        "def load_and_preprocess_image(img_path, target_size=(48, 48), grayscale=False):\n",
        "    if grayscale:\n",
        "        img = image.load_img(img_path, target_size=target_size, color_mode='grayscale')\n",
        "    else:\n",
        "        img = image.load_img(img_path, target_size=target_size)\n",
        "\n",
        "    img_array = image.img_to_array(img) # Convert image to array\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array /= 255.0  # Rescale to [0, 1]\n",
        "    return img_array\n",
        "\n",
        "# Function to predict the class of the image\n",
        "def predict_emotion(img_path, model, model_name):\n",
        "    class_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "    img_array = load_and_preprocess_image(img_path, grayscale=(model_name == 'CNN'))\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class = np.argmax(predictions)\n",
        "    return class_labels[predicted_class], np.max(predictions)\n",
        "\n",
        "# Upload and predict\n",
        "uploaded = files.upload()\n",
        "for img_path in uploaded.keys():\n",
        "    # Display the image\n",
        "    img = mpimg.imread(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Predict the emotion using CNN model\n",
        "    predicted_emotion, confidence = predict_emotion(img_path, cnn_model, 'CNN')\n",
        "    print(f'CNN Model Prediction: The predicted emotion for {img_path} is: {predicted_emotion} with confidence {confidence*100:.2f}%')\n",
        "\n",
        "    # Predict the emotion using VGG16 model\n",
        "    predicted_emotion, confidence = predict_emotion(img_path, vgg16_model, 'VGG16')\n",
        "    print(f'VGG16 Model Prediction: The predicted emotion for {img_path} is: {predicted_emotion} with confidence {confidence*100:.2f}%')\n",
        "\n",
        "    # Predict the emotion using ResNet model\n",
        "    predicted_emotion, confidence = predict_emotion(img_path, resnet_model, 'ResNet')\n",
        "    print(f'ResNet Model Prediction: The predicted emotion for {img_path} is: {predicted_emotion} with confidence {confidence*100:.2f}%')\n"
      ],
      "metadata": {
        "id": "-DZkVHzg1Doo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "This project demonstrates the effectiveness of advanced neural network architectures (VGG16 and ResNet50) in facial expression recognition, outperforming simpler CNN models and baseline models like KNN, SVM, and Random Forest. The use of transfer learning with pre-trained models like VGG16 and ResNet50 significantly enhances the accuracy and robustness of the emotion recognition system.\n",
        "\n",
        "This comparative study underscores the importance of choosing the right model architecture for specific computer vision tasks and highlights the potential of deep learning in understanding and interpreting human emotions."
      ],
      "metadata": {
        "id": "BShQX0ol0up4"
      }
    }
  ]
}